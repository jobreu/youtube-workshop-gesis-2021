---
title: "Automatic Sampling and Analysis of YouTube Data"
subtitle: "Excursus Screenscraping"
author: "Julian Kohne<br />Johannes Breuer<br />M. Rohangis Mohseni"
date: "2020-02-10"
location: "GESIS, Cologne, Germany"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "default-fonts", "../workshop.css"]
    nature:
      highlightStyle: "github"
      highlightLines: true
      countIncrementalSlides: false
---

```{r, echo=FALSE,results='hide'}
library(emo)

```


# Screenscraping

There are basically two ways to do screenscraping R
  - Rvest: Sufficient for scraping **static** websites
  - RSelenium: Can also deal with **dynamic** websites

Dynamic websites are pages that _dynamically_ load content from the database without
changing the URL

Example: When you click on "show more" on the comment replies of a _YouTube_ video, new content
is loaded from the database but not the whole website is reloaded. This is done with [Ajax](https://en.wikipedia.org/wiki/Ajax_(programming) )

---
# Screenscraping with Rvest

```{r collapse = TRUE, results='hide'}
# installing and attaching package
if ("rvest" %in% installed.packages() != TRUE) {
  install.packages("rvest")};library(rvest)

# defining website and Xpath from inspect function in browser
page <- "https://www.youtube.com/watch?v=1aheRpmurAo&"
Xp <- "/html/body/div[2]/div[4]/div/div[5]
/div[2]/div[2]/div/div[2]/meta[2]"

# getting page
Website <- read_html(page)

# getting node containing the description
Description <- html_nodes(Website, xpath = Xp)

# printing description
html_attr(Description, name = "content")
```
"John Oliver discusses the census, why it matters, and the consequences of an undercount.
Connect with Last Week Tonight online.Subscribe to the Last Week ..."

---
# Screenscraping with RSelenium

```{r}
# We first have to configure docker and open a docker container:
# https://callumgwtaylor.github.io/blog/2018/02/01/
# using-rselenium-and-docker-to-webscrape-in-r-using-the
# -who-snake-database/

# installing packages
if ("RSelenium" %in% installed.packages() != TRUE) {
  install.packages("RSelenium")
}

# attaching package
library(RSelenium)

# opening docker container from system
check <- system2("docker", args = "ps", stdout = TRUE)

```

---
# Screenscraping with RSelenium

```{r}

# opening new container (and killing old ones)
if (length(check) == 1) {
  
  #start new container
  system2("docker", args = c("run",
                             "-d",
                             "-p",
                             "4445:4444",
                             "selenium/standalone-chrome"))
  
} else {
  
  # kill old container
  DockerName <- trimws(strsplit(check[2],"tcp")[[1]][2])
  system2("docker", args = c("kill",DockerName))
  
  # start new container
  system2("docker", args = c("run",
                             "-d",
                             "-p",
                             "4445:4444",
                             "selenium/standalone-chrome"))
  
}

```
---
# Screenscraping with RSelenium

```{r}
# Assigning google chrome docker session
remDr <- RSelenium::remoteDriver(remoteServerAddr = "localhost",
                                 port = 4445L,
                                 browserName = "chrome")

# Waiting for 5 seconds to finish initialization of docker session
Sys.sleep(5)

```

---
# Screenscraping with RSelenium

- We can now navigate to a website and print a screenshot

```{r, results = 'hide'}

# Open remote connection
remDr$open()

# Navigate to website
remDr$navigate("https://www.youtube.com/watch?v=1aheRpmurAo&")

# Wait for 2 seconds for the website to load
Sys.sleep(2)

# Scrolling down a bit
webElem <- remDr$findElement("css", "body")
for (i in 20){
  webElem$sendKeysToElement(list(key = "down_arrow"))
}


# take screenshot
remDr$screenshot(file = 'Images/RSeleniumScreenshot.png')
```

---
# Screenshot 

![plot](Images/RSeleniumScreenshot.png)

---
# Screenscraping with RSelenium

 - We can then navigate to the "show more" button, and click it

```{r, results = 'hide'}

# Xpath of "show more" button (using inspect element in Browser)
xp <- '//*[@id="more"]/yt-formatted-string'

# navigating to button element
element <- remDr$findElement(using = 'xpath', xp)

# click on button
element$clickElement()

# Scrolling down a bit
webElem <- remDr$findElement("css", "body")
for (i in 20){
  webElem$sendKeysToElement(list(key = "down_arrow"))
}

# take screenshot (we can see that the description box
# is now expanded)
remDr$screenshot(file = 'Images/RSeleniumScreenshot2.png')

```
---
# Screenshot 

![plot](Images/RSeleniumScreenshot2.png)

---
# Screenscraping with RSelenium

- We can then extract the contents of the expanded description box

```{r, results = 'hide'}

#navigating to description element
xp2 <- '//*[@id="description"]/yt-formatted-string'
element2 <- remDr$findElement(using = 'xpath', xp2)

# get element text
unlist(element2$getElementText())

```

"John Oliver discusses the census, why it matters, and the consequences of an undercount.\n\nConnect with Last Week Tonight online... \n\nSubscribe to the Last Week Tonight YouTube channel for more almost news as it almost happens: www.youtube.com/lastweektonight \n\nFind Last Week Tonight on Facebook like your mom would: www.facebook.com/lastweektonight \n\nFollow us on Twitter for news about jokes and jokes about news: www.twitter.com/lastweektonight \n\nVisit our official site for all that other stuff at once: www.hbo.com/lastweektonight"

---
class: center, middle

# [Exercise](https://jobreu.github.io/youtube-workshop-gesis-2020/exercises/A2_Excursus_Screenscraping_question.html) time `r ji("weight_lifting_woman")``r ji("muscle")``r ji("running_man")``r ji("biking_man")`

## [Solutions](https://jobreu.github.io/youtube-workshop-gesis-2020/solutions/A2_Excursus_Screenscraping_solution.html)
